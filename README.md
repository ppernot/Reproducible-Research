# Reproducible Research

Codes and data to reproduce the results of the following papers:

* [2024_SimRef](https://github.com/ppernot/2024_SimRef)
"Validation of ML-UQ calibration statistics using simulated reference values: a sensitivity analysis" by P. Pernot (2024) [arXiv](https://doi.org/10.48550/arXiv.2403.00423)

* [2024_RCE](https://github.com/ppernot/2024_RCE)
"Negative impact of heavy-tailed uncertainty and
error distributions on the reliability of calibration
statistics for machine learning regression tasks"
by P. Pernot (2024) [arXiv](https://doi.org/10.48550/arXiv.2402.10043) 

* [2023_BVS](https://github.com/ppernot/2023_BVS)
"Can bin-wise scaling improve consistency and adaptivity of prediction uncertainty for machine learning regression ?"
by P. Pernot (2023) [arXiv](https://doi.org/10.48550/arXiv.2310.11978)

* [2023_Adaptivity](https://github.com/ppernot/2023_Adaptivity)
"Calibration in Machine Learning Uncertainty Quantification: beyond consistency to target adaptivity" by P. Pernot (2023)
[_APL Mach. Learn._ __1__:046121](https://doi.org/10.1063/5.0174943); [arXiv](https://doi.org/10.48550/arXiv.2309.06240)

* [2023_isotonic](https://github.com/ppernot/isotonic)
"Stratification of uncertainties recalibrated by isotonic regression and its impact on calibration error statistics" by P. Pernot (2023)  [arXiv](https://doi.org/10.48550/arXiv.2306.05180)

* [2023_ENCE](https://github.com/ppernot/2023_ENCE)
"Properties of the ENCE and other MAD-based calibration metrics"
by P. Pernot (2023) [arXiv](https://doi.org/10.48550/arXiv.2305.11905)

* [2023_Primer](https://github.com/ppernot/2023_Primer)
"Validation of uncertainty quantification metrics: a primer based on the consistency and adaptivity concepts" by P. Pernot (2023)
[arXiv](https://doi.org/10.48550/arXiv.2303.07170)

* [2022_SampleMean](https://github.com/ppernot/2022_SampleMean)
"Comparison of recent estimators of uncertainty 
on the mean for small measurement samples with 
normal and non-normal error distributions"
by P. Pernot and J.-P. Berthet (2022) [arXiv](https://doi.org/10.48550/arXiv.2209.04180)

* [2022_Confidence](https://github.com/ppernot/2022_Confidence)
"Confidence curves for UQ validation: probabilistic reference vs. oracle" by P. Pernot (2022) [arXiv](https://arxiv.org/abs/2206.15272v1)

* [2022_Tightness](https://github.com/ppernot/2022_Tightness)
"Prediction uncertainty validation for computational chemists" by P. Pernot (2022)
[_J. Chem. Phys._ __157__:144103](https://doi.org/10.1063/5.0109572); 
[arXiv](https://doi.org/10.48550/arXiv.2204.13477)

* [PU2022](https://github.com/ppernot/PU2022)
"The long road to calibrated prediction uncertainty in computational chemistry"
by P. Pernot  (2022) [_J. Chem. Phys._ __156__:114109](https://doi.org/10.1063/5.0084302); [arXiv](https://arxiv.org/abs/2201.01511)

* [Gini](https://github.com/ppernot/Gini) 
"Using the Gini coefficient to characterize the shape of computational chemistry error distributions", by P. Pernot and A. Savin (2021) [_Theor. Chem. Acc._ __140__:24](https://doi.org/10.1007/s00214-021-02725-0); [arXiv](https://arxiv.org/abs/2012.09589) 
  
* [ML2020](https://github.com/ppernot/ML2020)
"Impact of non-normal error distributions on the benchmarking and ranking of Quantum Machine Learning models", by P. Pernot, B. Huang and A. Savin (2020) [_Machine Learning: Science and Technology_ __1__:035011](https://doi.org/10.1088/2632-2153/aba184); [arXiv](https://arxiv.org/abs/2004.02524)

* [SIP](https://github.com/ppernot/SIP)
  "Probabilistic performance estimators for computational chemistry methods: 
  Systematic Improvement Probability and Ranking Probability Matrix. I. Theory", 
  by P. Pernot and A. Savin (2020).
  [_J. Chem. Phys._ __152__:164108](http://dx.doi.org/10.1063/5.0006202);
  [arXiv](https://arxiv.org/abs/2003.00987),    
  and    
  "Probabilistic performance estimators for computational chemistry methods: 
  Systematic Improvement Probability and Ranking Probability Matrix. II. Applications", 
  by P. Pernot and A. Savin (2020).
  [_J. Chem. Phys._ __152__:164109](http://dx.doi.org/10.1063/5.0006204);
  [arXiv](https://arxiv.org/abs/2003.01572).

* [ECDFT](https://github.com/ppernot/ECDFT) 
  "Probabilistic performance estimators for computational chemistry methods: the empirical cumulative distribution function of absolute errors", by P. Pernot and A. Savin (2018) [_J. Chem. Phys._ __148__:241707](http://dx.doi.org/10.1063/1.5016248)     
  
* [PUIF](https://github.com/ppernot/PUIF) 
"The parameters uncertainty inflation fallacy", by P. Pernot (2017) [_J. Chem. Phys._ __147__:104102](http://dx.doi.org/10.1063/1.4994654); [arXiv](https://arxiv.org/abs/1611.04295)
  
* [CalPred](https://github.com/ppernot/CalPred) 
"A critical review of statistical calibration/prediction models handling data inconsistency and model  inadequacy", by P. Pernot and F. Cailliez (2017) [_AIChE J._ __63__:4642](http://dx.doi.org/10.1002/aic.15781); [arXiv](https://arxiv.org/abs/1611.04376)

* [PU_DFA](./PU_DFA)
"Prediction Uncertainty of Density Functional Approximations for Properties of Crystals with Cubic Symmetry" by P. Pernot, B. Civalleri, D. Presti and A. Savin (2015) [_J. Phys. Chem. A_ __119__:5288-5304](http://dx.doi.org/10.1021/jp509980w)

* [NegVar](./NegVar)
"Model's output variance can increase when input variance decreases: a sensitivity analysis paradox?" by P. Pernot, M. DÃ©senfant and F. Hennebelle (2015) [17th International Congress of Metrology, B. Larquier, Ed., EDP Sciences](http://dx.doi.org/10.1051/metrology/20150002004)
